{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   document                                          full_text  \\\n",
      "0         7  Design Thinking for innovation reflexion-Avril...   \n",
      "1        10  Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...   \n",
      "2        16  Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...   \n",
      "3        20  Design Thinking for Innovation\\n\\nSindy Samaca...   \n",
      "4        56  Assignment:  Visualization Reflection  Submitt...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [Design, Thinking, for, innovation, reflexion,...   \n",
      "1  [Diego, Estrada, \\n\\n, Design, Thinking, Assig...   \n",
      "2  [Reporting, process, \\n\\n, by, Gilberto, Gambo...   \n",
      "3  [Design, Thinking, for, Innovation, \\n\\n, Sind...   \n",
      "4  [Assignment, :,   , Visualization,  , Reflecti...   \n",
      "\n",
      "                                 trailing_whitespace  \\\n",
      "0  [True, True, True, True, False, False, True, F...   \n",
      "1  [True, False, False, True, True, False, False,...   \n",
      "2  [True, False, False, True, True, False, False,...   \n",
      "3  [True, True, True, False, False, True, False, ...   \n",
      "4  [False, False, False, False, False, False, Fal...   \n",
      "\n",
      "                                              labels  \n",
      "0  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...  \n",
      "1  [B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...  \n",
      "2  [O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...  \n",
      "3  [O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...  \n",
      "4  [O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...  \n",
      "   document                                          full_text  \\\n",
      "0         7  Design Thinking for innovation reflexion-Avril...   \n",
      "1        10  Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...   \n",
      "2        16  Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...   \n",
      "3        20  Design Thinking for Innovation\\n\\nSindy Samaca...   \n",
      "4        56  Assignment:  Visualization Reflection  Submitt...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [Design, Thinking, for, innovation, reflexion,...   \n",
      "1  [Diego, Estrada, \\n\\n, Design, Thinking, Assig...   \n",
      "2  [Reporting, process, \\n\\n, by, Gilberto, Gambo...   \n",
      "3  [Design, Thinking, for, Innovation, \\n\\n, Sind...   \n",
      "4  [Assignment, :,   , Visualization,  , Reflecti...   \n",
      "\n",
      "                                 trailing_whitespace  \n",
      "0  [True, True, True, True, False, False, True, F...  \n",
      "1  [True, False, False, True, True, False, False,...  \n",
      "2  [True, False, False, True, True, False, False,...  \n",
      "3  [True, True, True, False, False, True, False, ...  \n",
      "4  [False, False, False, False, False, False, Fal...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "# Load the datasets\n",
    "train_data_path = './train.json' # Update this path\n",
    "test_data_path = './test.json'   # Update this path\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open(train_data_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    # Convert the loaded JSON data into a pandas DataFrame\n",
    "    train_df = pd.DataFrame(data)\n",
    "with open(test_data_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    # Convert the loaded JSON data into a pandas DataFrame\n",
    "    test_df = pd.DataFrame(data)\n",
    "# Quick look at the data structure\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PII_Type\n",
      "O                   4989794\n",
      "B-NAME_STUDENT         1365\n",
      "I-NAME_STUDENT         1096\n",
      "B-URL_PERSONAL          110\n",
      "B-ID_NUM                 78\n",
      "B-EMAIL                  39\n",
      "I-STREET_ADDRESS         20\n",
      "I-PHONE_NUM              15\n",
      "B-USERNAME                6\n",
      "B-PHONE_NUM               6\n",
      "B-STREET_ADDRESS          2\n",
      "I-URL_PERSONAL            1\n",
      "I-ID_NUM                  1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pii_types = [label for sublist in train_df['labels'].tolist() for label in sublist]\n",
    "pii_types_df = pd.DataFrame(pii_types, columns=['PII_Type'])\n",
    "\n",
    "# View the distribution of PII types\n",
    "print(pii_types_df['PII_Type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'design', 'thinking', 'for', 'innovation', 'reflex', '##ion', '-', 'av', '##ril', '2021', '-', 'nat', '##hal', '##ie', 'sy', '##lla', 'challenge', '&', 'selection', 'the', 'tool', 'i', 'use', 'to', 'help', 'all', 'stakeholders', 'finding', 'their', 'way', 'through', 'the', 'complexity', 'of', 'a', 'project', 'is', 'the', 'mind', 'map', '.', 'what', 'exactly', 'is', 'a', 'mind', 'map', '?', 'according', 'to', 'the', 'definition', 'of', 'bu', '##zan', 't', '.', 'and', 'bu', '##zan', 'b', '.', '(', '1999', ',', 'des', '##sin', '##e', '-', 'moi', 'l', \"'\", 'intelligence', '.', 'paris', ':', 'les', 'editions', 'd', \"'\", 'organisation', '.', ')', ',', 'the', 'mind', 'map', '(', 'or', 'he', '##uri', '##stic', 'diagram', ')', 'is', 'a', 'graphic', 'representation', 'technique', 'that', 'follows', 'the', 'natural', 'functioning', 'of', 'the', 'mind', 'and', 'allows', 'the', 'brain', \"'\", 's', 'potential', 'to', 'be', 'released', '.', 'cf', 'annex', '##1', 'this', 'tool', 'has', 'many', 'advantages', ':', '•', 'it', 'is', 'accessible', 'to', 'all', 'and', 'does', 'not', 'require', 'significant', 'material', 'investment', 'and', 'can', 'be', 'done', 'quickly', '•', 'it', 'is', 'scala', '##ble', '•', 'it', 'allows', 'cat', '##ego', '##rization', 'and', 'linking', 'of', 'information', '•', 'it', 'can', 'be', 'applied', 'to', 'any', 'type', 'of', 'situation', ':', 'note', '##taking', ',', 'problem', 'solving', ',', 'analysis', ',', 'creation', 'of', 'new', 'ideas', '•', 'it', 'is', 'suitable', 'for', 'all', 'people', 'and', 'is', 'easy', 'to', 'learn', '•', 'it', 'is', 'fun', 'and', 'encourages', 'exchanges', '•', 'it', 'makes', 'visible', 'the', 'dimension', 'of', 'projects', ',', 'opportunities', ',', 'inter', '##con', '##ne', '##ctions', '•', 'it', 'synth', '##es', '##izes', '•', 'it', 'makes', 'the', 'project', 'understand', '##able', '•', 'it', 'allows', 'you', 'to', 'explore', 'ideas', 'the', 'creation', 'of', 'a', 'mind', 'map', 'starts', 'with', 'an', 'idea', '/', 'problem', 'located', 'at', 'its', 'center', '.', 'this', 'starting', 'point', 'generates', 'ideas', '/', 'work', 'areas', ',', 'inc', '##rem', '##ented', 'around', 'this', 'center', 'in', 'a', 'radial', 'structure', ',', 'which', 'in', 'turn', 'is', 'completed', 'with', 'as', 'many', 'branches', 'as', 'new', 'ideas', '.', 'this', 'tool', 'enables', 'creativity', 'and', 'logic', 'to', 'be', 'mobilized', ',', 'it', 'is', 'a', 'map', 'of', 'the', 'thoughts', '.', 'creativity', 'is', 'enhanced', 'because', 'participants', 'feel', 'comfortable', 'with', 'the', 'method', '.', 'application', '&', 'insight', 'i', 'start', 'the', 'process', 'of', 'the', 'mind', 'map', 'creation', 'with', 'the', 'stakeholders', 'standing', 'around', 'a', 'large', 'board', '(', 'white', 'or', 'paper', 'board', ')', '.', 'in', 'the', 'center', 'of', 'the', 'board', ',', 'i', 'write', 'and', 'highlight', 'the', 'topic', 'to', 'design', '.', 'through', 'a', 'series', 'of', 'questions', ',', 'i', 'guide', 'the', 'stakeholders', 'in', 'modelling', 'the', 'mind', 'map', '.', 'i', 'adapt', 'the', 'series', 'of', 'questions', 'according', 'to', 'the', 'topic', 'to', 'be', 'addressed', '.', 'in', 'the', 'type', 'of', 'questions', ',', 'we', 'can', 'use', ':', 'who', ',', 'what', ',', 'when', ',', 'where', ',', 'why', ',', 'how', ',', 'how', 'much', '.', 'the', 'use', 'of', 'the', '“', 'why', '”', 'is', 'very', 'interesting', 'to', 'understand', 'the', 'origin', '.', 'by', 'this', 'way', ',', 'the', 'interviewed', 'person', 'free', '##s', 'itself', 'from', 'paradigm', '##s', 'and', 'thus', 'dare', '##s', 'to', 'propose', 'new', 'ideas', '/', 'ways', 'of', 'functioning', '.', 'i', 'plan', 'two', 'hours', 'for', 'a', 'workshop', '.', 'design', 'thinking', 'for', 'innovation', 'reflex', '##ion', '-', 'av', '##ril', '2021', '-', 'nat', '##hal', '##ie', 'sy', '##lla', 'after', 'modelling', 'the', 'mind', 'map', 'on', 'paper', ',', 'i', 'propose', 'to', 'the', 'participants', 'a', 'digital', 'visual', '##ization', 'of', 'their', 'work', 'with', 'the', 'addition', 'of', 'color', 'codes', ',', 'images', 'and', 'inter', '##con', '##ne', '[SEP]']\n",
      "512\n",
      "[101, 2640, 3241, 2005, 8144, 22259, 3258, 1011, 20704, 15928, 25682, 1011, 14085, 8865, 2666, 25353, 4571, 4119, 1004, 4989, 1996, 6994, 1045, 2224, 2000, 2393, 2035, 22859, 4531, 2037, 2126, 2083, 1996, 11619, 1997, 1037, 2622, 2003, 1996, 2568, 4949, 1012, 2054, 3599, 2003, 1037, 2568, 4949, 1029, 2429, 2000, 1996, 6210, 1997, 20934, 13471, 1056, 1012, 1998, 20934, 13471, 1038, 1012, 1006, 2639, 1010, 4078, 11493, 2063, 1011, 25175, 1048, 1005, 4454, 1012, 3000, 1024, 4649, 6572, 1040, 1005, 5502, 1012, 1007, 1010, 1996, 2568, 4949, 1006, 2030, 2002, 9496, 10074, 16403, 1007, 2003, 1037, 8425, 6630, 6028, 2008, 4076, 1996, 3019, 12285, 1997, 1996, 2568, 1998, 4473, 1996, 4167, 1005, 1055, 4022, 2000, 2022, 2207, 1012, 12935, 17827, 2487, 2023, 6994, 2038, 2116, 12637, 1024, 1528, 2009, 2003, 7801, 2000, 2035, 1998, 2515, 2025, 5478, 3278, 3430, 5211, 1998, 2064, 2022, 2589, 2855, 1528, 2009, 2003, 26743, 3468, 1528, 2009, 4473, 4937, 20265, 26910, 1998, 11383, 1997, 2592, 1528, 2009, 2064, 2022, 4162, 2000, 2151, 2828, 1997, 3663, 1024, 3602, 17904, 1010, 3291, 13729, 1010, 4106, 1010, 4325, 1997, 2047, 4784, 1528, 2009, 2003, 7218, 2005, 2035, 2111, 1998, 2003, 3733, 2000, 4553, 1528, 2009, 2003, 4569, 1998, 16171, 15800, 1528, 2009, 3084, 5710, 1996, 9812, 1997, 3934, 1010, 6695, 1010, 6970, 8663, 2638, 22014, 1528, 2009, 24203, 2229, 10057, 1528, 2009, 3084, 1996, 2622, 3305, 3085, 1528, 2009, 4473, 2017, 2000, 8849, 4784, 1996, 4325, 1997, 1037, 2568, 4949, 4627, 2007, 2019, 2801, 1013, 3291, 2284, 2012, 2049, 2415, 1012, 2023, 3225, 2391, 19421, 4784, 1013, 2147, 2752, 1010, 4297, 28578, 14088, 2105, 2023, 2415, 1999, 1037, 15255, 3252, 1010, 2029, 1999, 2735, 2003, 2949, 2007, 2004, 2116, 5628, 2004, 2047, 4784, 1012, 2023, 6994, 12939, 14842, 1998, 7961, 2000, 2022, 27526, 1010, 2009, 2003, 1037, 4949, 1997, 1996, 4301, 1012, 14842, 2003, 9412, 2138, 6818, 2514, 6625, 2007, 1996, 4118, 1012, 4646, 1004, 12369, 1045, 2707, 1996, 2832, 1997, 1996, 2568, 4949, 4325, 2007, 1996, 22859, 3061, 2105, 1037, 2312, 2604, 1006, 2317, 2030, 3259, 2604, 1007, 1012, 1999, 1996, 2415, 1997, 1996, 2604, 1010, 1045, 4339, 1998, 12944, 1996, 8476, 2000, 2640, 1012, 2083, 1037, 2186, 1997, 3980, 1010, 1045, 5009, 1996, 22859, 1999, 19518, 1996, 2568, 4949, 1012, 1045, 15581, 1996, 2186, 1997, 3980, 2429, 2000, 1996, 8476, 2000, 2022, 8280, 1012, 1999, 1996, 2828, 1997, 3980, 1010, 2057, 2064, 2224, 1024, 2040, 1010, 2054, 1010, 2043, 1010, 2073, 1010, 2339, 1010, 2129, 1010, 2129, 2172, 1012, 1996, 2224, 1997, 1996, 1523, 2339, 1524, 2003, 2200, 5875, 2000, 3305, 1996, 4761, 1012, 2011, 2023, 2126, 1010, 1996, 10263, 2711, 2489, 2015, 2993, 2013, 20680, 2015, 1998, 2947, 8108, 2015, 2000, 16599, 2047, 4784, 1013, 3971, 1997, 12285, 1012, 1045, 2933, 2048, 2847, 2005, 1037, 8395, 1012, 2640, 3241, 2005, 8144, 22259, 3258, 1011, 20704, 15928, 25682, 1011, 14085, 8865, 2666, 25353, 4571, 2044, 19518, 1996, 2568, 4949, 2006, 3259, 1010, 1045, 16599, 2000, 1996, 6818, 1037, 3617, 5107, 3989, 1997, 2037, 2147, 2007, 1996, 2804, 1997, 3609, 9537, 1010, 4871, 1998, 6970, 8663, 2638, 102]\n",
      "[(0, 0), (0, 6), (7, 15), (16, 19), (20, 30), (31, 37), (37, 40), (40, 41), (41, 43), (43, 46), (47, 51), (51, 52), (52, 55), (55, 58), (58, 60), (61, 63), (63, 66), (68, 77), (78, 79), (80, 89), (91, 94), (95, 99), (100, 101), (102, 105), (106, 108), (109, 113), (114, 117), (118, 130), (131, 138), (139, 144), (145, 148), (149, 156), (157, 160), (161, 171), (172, 174), (175, 176), (177, 184), (185, 187), (188, 191), (193, 197), (198, 201), (201, 202), (204, 208), (209, 216), (217, 219), (220, 221), (222, 226), (227, 230), (230, 231), (232, 241), (242, 244), (245, 248), (249, 259), (260, 262), (263, 265), (265, 268), (269, 270), (270, 271), (272, 275), (276, 278), (278, 281), (282, 283), (283, 284), (285, 286), (286, 290), (290, 291), (292, 295), (295, 298), (298, 299), (299, 300), (300, 303), (305, 306), (306, 307), (307, 319), (319, 320), (321, 326), (326, 327), (328, 331), (332, 340), (341, 342), (342, 343), (343, 355), (355, 356), (356, 357), (357, 358), (359, 362), (363, 367), (368, 371), (372, 373), (373, 375), (376, 378), (378, 381), (381, 385), (386, 393), (393, 394), (395, 397), (398, 399), (400, 407), (409, 423), (424, 433), (434, 438), (439, 446), (447, 450), (451, 458), (459, 470), (471, 473), (474, 477), (478, 482), (483, 486), (487, 493), (494, 497), (498, 503), (503, 504), (504, 505), (507, 516), (517, 519), (520, 522), (523, 531), (531, 532), (533, 535), (536, 541), (541, 542), (544, 548), (549, 553), (554, 557), (558, 562), (563, 573), (573, 574), (576, 577), (579, 581), (582, 584), (585, 595), (596, 598), (599, 602), (603, 606), (607, 611), (612, 615), (616, 623), (624, 635), (636, 644), (645, 655), (656, 659), (660, 663), (664, 666), (667, 671), (673, 680), (682, 683), (685, 687), (688, 690), (691, 696), (696, 699), (701, 702), (704, 706), (707, 713), (714, 717), (717, 720), (720, 728), (729, 732), (733, 740), (741, 743), (744, 755), (757, 758), (760, 762), (763, 766), (767, 769), (770, 777), (778, 780), (781, 784), (785, 789), (790, 792), (793, 802), (802, 803), (804, 808), (808, 814), (814, 815), (816, 823), (824, 831), (831, 832), (833, 841), (841, 842), (843, 851), (852, 854), (856, 859), (860, 865), (867, 868), (870, 872), (873, 875), (876, 884), (885, 888), (889, 892), (893, 899), (900, 903), (904, 906), (907, 911), (912, 914), (915, 920), (922, 923), (925, 927), (928, 930), (931, 934), (935, 938), (939, 949), (950, 959), (961, 962), (964, 966), (967, 972), (973, 980), (981, 984), (985, 994), (995, 997), (998, 1006), (1006, 1007), (1008, 1021), (1021, 1022), (1023, 1028), (1028, 1031), (1031, 1033), (1033, 1039), (1041, 1042), (1044, 1046), (1047, 1052), (1052, 1054), (1054, 1058), (1060, 1061), (1063, 1065), (1066, 1071), (1072, 1075), (1076, 1083), (1084, 1094), (1094, 1098), (1100, 1101), (1103, 1105), (1106, 1112), (1113, 1116), (1117, 1119), (1120, 1127), (1128, 1133), (1135, 1138), (1139, 1147), (1148, 1150), (1151, 1152), (1153, 1157), (1158, 1161), (1162, 1168), (1169, 1173), (1174, 1176), (1177, 1181), (1181, 1182), (1182, 1189), (1190, 1197), (1198, 1200), (1201, 1204), (1205, 1211), (1211, 1212), (1213, 1217), (1218, 1226), (1227, 1232), (1234, 1243), (1244, 1249), (1249, 1250), (1250, 1254), (1255, 1260), (1260, 1261), (1262, 1265), (1265, 1268), (1268, 1273), (1274, 1280), (1281, 1285), (1286, 1292), (1293, 1295), (1296, 1297), (1298, 1304), (1305, 1314), (1314, 1315), (1316, 1321), (1322, 1324), (1325, 1329), (1330, 1332), (1334, 1343), (1344, 1348), (1349, 1351), (1352, 1356), (1357, 1365), (1366, 1368), (1369, 1372), (1373, 1378), (1378, 1379), (1381, 1385), (1386, 1390), (1391, 1398), (1399, 1409), (1410, 1413), (1414, 1419), (1420, 1422), (1423, 1425), (1426, 1435), (1435, 1436), (1437, 1439), (1440, 1442), (1443, 1444), (1445, 1448), (1449, 1451), (1452, 1455), (1456, 1464), (1464, 1465), (1467, 1477), (1478, 1480), (1481, 1489), (1490, 1497), (1498, 1510), (1511, 1515), (1516, 1527), (1528, 1532), (1533, 1536), (1537, 1543), (1543, 1544), (1546, 1557), (1558, 1559), (1560, 1567), (1569, 1570), (1571, 1576), (1577, 1580), (1581, 1588), (1589, 1591), (1592, 1595), (1596, 1600), (1601, 1604), (1605, 1613), (1614, 1618), (1619, 1622), (1623, 1635), (1636, 1644), (1645, 1651), (1652, 1653), (1654, 1659), (1660, 1665), (1667, 1668), (1668, 1673), (1674, 1676), (1677, 1682), (1683, 1688), (1688, 1689), (1689, 1690), (1691, 1693), (1694, 1697), (1698, 1704), (1705, 1707), (1708, 1711), (1712, 1717), (1717, 1718), (1719, 1720), (1721, 1726), (1727, 1730), (1731, 1740), (1741, 1744), (1745, 1750), (1751, 1753), (1754, 1760), (1760, 1761), (1763, 1770), (1771, 1772), (1773, 1779), (1780, 1782), (1783, 1792), (1792, 1793), (1794, 1795), (1796, 1801), (1802, 1805), (1806, 1818), (1819, 1821), (1822, 1831), (1832, 1835), (1836, 1840), (1841, 1844), (1844, 1845), (1846, 1847), (1848, 1853), (1854, 1857), (1858, 1864), (1866, 1868), (1869, 1878), (1879, 1888), (1889, 1891), (1892, 1895), (1896, 1901), (1902, 1904), (1905, 1907), (1908, 1917), (1917, 1918), (1919, 1921), (1922, 1925), (1926, 1930), (1931, 1933), (1934, 1943), (1943, 1944), (1945, 1947), (1948, 1951), (1952, 1955), (1955, 1956), (1957, 1960), (1960, 1961), (1962, 1966), (1966, 1967), (1969, 1973), (1973, 1974), (1975, 1980), (1980, 1981), (1982, 1985), (1985, 1986), (1987, 1990), (1990, 1991), (1992, 1995), (1996, 2000), (2000, 2001), (2003, 2006), (2007, 2010), (2011, 2013), (2014, 2017), (2018, 2019), (2019, 2022), (2022, 2023), (2024, 2026), (2027, 2031), (2032, 2043), (2044, 2046), (2047, 2057), (2058, 2061), (2062, 2068), (2068, 2069), (2070, 2072), (2073, 2077), (2078, 2081), (2081, 2082), (2083, 2086), (2087, 2098), (2099, 2105), (2107, 2111), (2111, 2112), (2113, 2119), (2120, 2124), (2125, 2133), (2133, 2134), (2135, 2138), (2139, 2143), (2144, 2148), (2148, 2149), (2150, 2152), (2153, 2160), (2161, 2164), (2165, 2170), (2171, 2172), (2173, 2177), (2178, 2180), (2181, 2192), (2192, 2193), (2194, 2195), (2196, 2200), (2201, 2204), (2206, 2211), (2212, 2215), (2216, 2217), (2218, 2226), (2226, 2227), (2229, 2235), (2236, 2244), (2245, 2248), (2249, 2259), (2260, 2266), (2266, 2269), (2269, 2270), (2270, 2272), (2272, 2275), (2276, 2280), (2280, 2281), (2281, 2284), (2284, 2287), (2287, 2289), (2290, 2292), (2292, 2295), (2297, 2302), (2303, 2312), (2313, 2316), (2317, 2321), (2322, 2325), (2326, 2328), (2329, 2334), (2334, 2335), (2336, 2337), (2338, 2345), (2346, 2348), (2349, 2352), (2353, 2365), (2366, 2367), (2368, 2375), (2376, 2382), (2382, 2389), (2390, 2392), (2393, 2398), (2400, 2404), (2405, 2409), (2410, 2413), (2414, 2422), (2423, 2425), (2426, 2431), (2432, 2437), (2437, 2438), (2439, 2445), (2446, 2449), (2450, 2455), (2455, 2458), (2458, 2460), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode the text\n",
    "original_text = train_df['full_text'].iloc[0]\n",
    "original_tokens = train_df['tokens'].iloc[0]\n",
    "original_labels = train_df['labels'].iloc[0]\n",
    "\n",
    "full_text = original_text#\"innovation reflexion-Avril 2021-Nathalie Sylla\\n\\n\"\n",
    "tokens = original_tokens#[\"innovation\",\"reflexion\",\"-\",\"Avril\",\"2021\",\"-\",\"Nathalie\",\"Sylla\",\"\\n\\n\"]\n",
    "labels = original_labels#[\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"B-NAME_STUDENT\",\"I-NAME_STUDENT\",\"O\"]\n",
    "\n",
    "encoded = tokenizer.encode_plus(full_text, return_offsets_mapping=True, padding='max_length', truncation=True)\n",
    "new_tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'])\n",
    "# Print the result\n",
    "print(new_tokens)\n",
    "print(len(encoded['input_ids']))\n",
    "print(encoded['input_ids'])\n",
    "print(encoded['offset_mapping'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 1:\n",
      "Tokens: ['Design', 'Thinking', 'for', 'innovation', 'reflexion', '-', 'Avril', '2021', '-', 'Nathalie', 'Sylla', '\\n\\n', 'Challenge', '&', 'selection', '\\n\\n', 'The', 'tool', 'I', 'use', 'to', 'help', 'all', 'stakeholders', 'finding', 'their', 'way', 'through', 'the', 'complexity', 'of', 'a', 'project', 'is', 'the', ' ', 'mind', 'map', '.', '\\n\\n', 'What', 'exactly', 'is', 'a', 'mind', 'map', '?', 'According', 'to', 'the', 'definition', 'of', 'Buzan', 'T.', 'and', 'Buzan', 'B.', '(', '1999', ',', 'Dessine', '-', 'moi', ' ', \"l'intelligence\", '.', 'Paris', ':', 'Les', 'Éditions', \"d'Organisation\", '.', ')', ',', 'the', 'mind', 'map', '(', 'or', 'heuristic', 'diagram', ')', 'is', 'a', 'graphic', ' ', 'representation', 'technique', 'that', 'follows', 'the', 'natural', 'functioning', 'of', 'the', 'mind', 'and', 'allows', 'the', 'brain', \"'s\", ' ', 'potential', 'to', 'be', 'released', '.', 'Cf', 'Annex1', '\\n\\n', 'This', 'tool', 'has', 'many', 'advantages', ':', '\\n\\n', '•', ' ', 'It', 'is', 'accessible', 'to', 'all', 'and', 'does', 'not', 'require', 'significant', 'material', 'investment', 'and', 'can', 'be', 'done', ' ', 'quickly', '\\n\\n', '•', ' ', 'It', 'is', 'scalable', '\\n\\n', '•', ' ', 'It', 'allows', 'categorization', 'and', 'linking', 'of', 'information', '\\n\\n', '•', ' ', 'It', 'can', 'be', 'applied', 'to', 'any', 'type', 'of', 'situation', ':', 'notetaking', ',', 'problem', 'solving', ',', 'analysis', ',', 'creation', 'of', ' ', 'new', 'ideas', '\\n\\n', '•', ' ', 'It', 'is', 'suitable', 'for', 'all', 'people', 'and', 'is', 'easy', 'to', 'learn', '\\n\\n', '•', ' ', 'It', 'is', 'fun', 'and', 'encourages', 'exchanges', '\\n\\n', '•', ' ', 'It', 'makes', 'visible', 'the', 'dimension', 'of', 'projects', ',', 'opportunities', ',', 'interconnections', '\\n\\n', '•', ' ', 'It', 'synthesizes', '\\n\\n', '•', ' ', 'It', 'makes', 'the', 'project', 'understandable', '\\n\\n', '•', ' ', 'It', 'allows', 'you', 'to', 'explore', 'ideas', '\\n\\n', 'The', 'creation', 'of', 'a', 'mind', 'map', 'starts', 'with', 'an', 'idea', '/', 'problem', 'located', 'at', 'its', 'center', '.', 'This', 'starting', 'point', ' ', 'generates', 'ideas', '/', 'work', 'areas', ',', 'incremented', 'around', 'this', 'center', 'in', 'a', 'radial', 'structure', ',', 'which', 'in', 'turn', 'is', ' ', 'completed', 'with', 'as', 'many', 'branches', 'as', 'new', 'ideas', '.', '\\n\\n', 'This', 'tool', 'enables', 'creativity', 'and', 'logic', 'to', 'be', 'mobilized', ',', 'it', 'is', 'a', 'map', 'of', 'the', 'thoughts', '.', '\\n\\n', 'Creativity', 'is', 'enhanced', 'because', 'participants', 'feel', 'comfortable', 'with', 'the', 'method', '.', '\\n\\n', 'Application', '&', 'Insight', '\\n\\n', 'I', 'start', 'the', 'process', 'of', 'the', 'mind', 'map', 'creation', 'with', 'the', 'stakeholders', 'standing', 'around', 'a', 'large', 'board', ' ', '(', 'white', 'or', 'paper', 'board', ')', '.', 'In', 'the', 'center', 'of', 'the', 'board', ',', 'I', 'write', 'and', 'highlight', 'the', 'topic', 'to', 'design', '.', '\\n\\n', 'Through', 'a', 'series', 'of', 'questions', ',', 'I', 'guide', 'the', 'stakeholders', 'in', 'modelling', 'the', 'mind', 'map', '.', 'I', 'adapt', 'the', 'series', ' ', 'of', 'questions', 'according', 'to', 'the', 'topic', 'to', 'be', 'addressed', '.', 'In', 'the', 'type', 'of', 'questions', ',', 'we', 'can', 'use', ':', 'who', ',', 'what', ',', ' ', 'when', ',', 'where', ',', 'why', ',', 'how', ',', 'how', 'much', '.', '\\n\\n', 'The', 'use', 'of', 'the', '“', 'why', '”', 'is', 'very', 'interesting', 'to', 'understand', 'the', 'origin', '.', 'By', 'this', 'way', ',', 'the', 'interviewed', 'person', ' ', 'frees', 'itself', 'from', 'paradigms', 'and', 'thus', 'dares', 'to', 'propose', 'new', 'ideas', '/', 'ways', 'of', 'functioning', '.', 'I', 'plan', 'two', ' ', 'hours', 'for', 'a', 'workshop', '.', '\\n\\n', 'Design', 'Thinking', 'for', 'innovation', 'reflexion', '-', 'Avril', '2021', '-', 'Nathalie', 'Sylla', '\\n\\n', 'After', 'modelling', 'the', 'mind', 'map', 'on', 'paper', ',', 'I', 'propose', 'to', 'the', 'participants', 'a', 'digital', 'visualization', 'of', 'their', ' ', 'work', 'with', 'the', 'addition', 'of', 'color']\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Segment 2:\n",
      "Tokens: ['propose', 'to', 'the', 'participants', 'a', 'digital', 'visualization', 'of', 'their', ' ', 'work', 'with', 'the', 'addition', 'of', 'color', 'codes', ',', 'images', 'and', 'interconnections', '.', 'This', 'second', 'workshop', 'also', 'lasts', ' ', 'two', 'hours', 'and', 'allows', 'the', 'mind', 'map', 'to', 'evolve', '.', 'Once', 'familiarized', 'with', 'it', ',', 'the', 'stakeholders', 'discover', ' ', 'the', 'power', 'of', 'the', 'tool', '.', 'Then', ',', 'the', 'second', 'workshop', 'brings', 'out', 'even', 'more', 'ideas', 'and', 'constructive', ' ', 'exchanges', 'between', 'the', 'stakeholders', '.', 'Around', 'this', 'new', 'mind', 'map', ',', 'they', 'have', 'learned', 'to', 'work', ' ', 'together', 'and', 'want', 'to', 'make', 'visible', 'the', 'untold', 'ideas', '.', '\\n\\n', 'I', 'now', 'present', 'all', 'the', 'projects', 'I', 'manage', 'in', 'this', 'type', 'of', 'format', 'in', 'order', 'to', 'ease', 'rapid', 'understanding', 'for', ' ', 'decision', '-', 'makers', '.', 'These', 'presentations', 'are', 'the', 'core', 'of', 'my', 'business', 'models', '.', 'The', 'decision', '-', 'makers', 'are', ' ', 'thus', 'able', 'to', 'identify', 'the', 'opportunities', 'of', 'the', 'projects', 'and', 'can', 'take', 'quick', 'decisions', 'to', 'validate', 'them', '.', ' ', 'They', 'find', 'answers', 'to', 'their', 'questions', 'thank', 'to', 'a', 'schematic', 'representation', '.', '\\n\\n', 'Approach', '\\n\\n', 'What', 'I', 'find', 'amazing', 'with', 'the', 'facilitation', 'of', 'this', 'type', 'of', 'workshop', 'is', 'the', 'participants', 'commitment', 'for', ' ', 'the', 'project', '.', 'This', 'tool', 'helps', 'to', 'give', 'meaning', '.', 'The', 'participants', 'appropriate', 'the', 'story', 'and', 'want', 'to', 'keep', ' ', 'writing', 'it', '.', 'Then', ',', 'they', 'easily', 'become', 'actors', 'or', 'sponsors', 'of', 'the', 'project', '.', 'A', 'trust', 'relationship', 'is', 'built', ',', ' ', 'thus', 'facilitating', 'the', 'implementation', 'of', 'related', 'actions', '.', '\\n\\n', 'Design', 'Thinking', 'for', 'innovation', 'reflexion', '-', 'Avril', '2021', '-', 'Nathalie', 'Sylla', '\\n\\n', 'Annex', '1', ':', 'Mind', 'Map', 'Shared', 'facilities', 'project', '\\n\\n']\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def segment_text_by_tokens(original_tokens, original_labels, max_length, overlap):\n",
    "    # Adjust max_length to account for special tokens like [CLS] and [SEP] which are added by models like BERT\n",
    "    max_length -= 2  # Assuming 2 special tokens: [CLS] and [SEP]\n",
    "\n",
    "    segments = []\n",
    "    current_tokens = []\n",
    "    current_labels = []\n",
    "\n",
    "    for token, label in zip(original_tokens, original_labels):\n",
    "        # Check if adding the next token would exceed the max_length\n",
    "        if len(current_tokens) >= max_length and current_tokens:\n",
    "            # Finalize the current segment before the overflow\n",
    "            segments.append({\n",
    "                \"tokens\": current_tokens,\n",
    "                \"labels\": current_labels\n",
    "            })\n",
    "            # Start new segment with overlap if specified\n",
    "            current_tokens = current_tokens[-overlap:] if overlap else []\n",
    "            current_labels = current_labels[-overlap:] if overlap else []\n",
    "\n",
    "        current_tokens.append(token)\n",
    "        current_labels.append(label)\n",
    "    \n",
    "    # Add the last segment if there are remaining tokens\n",
    "    if current_tokens:\n",
    "        segments.append({\n",
    "            \"tokens\": current_tokens,\n",
    "            \"labels\": current_labels\n",
    "        })\n",
    "    \n",
    "    return segments\n",
    "\n",
    "# Example usage\n",
    "# original_text = \"innovation reflexion-Avril 2021-Nathalie Sylla\\n\\n\"\n",
    "# original_tokens = [\"innovation\",\"reflexion\",\"-\",\"Avril\",\"2021\",\"-\",\"Nathalie\",\"Sylla\",\"\\n\\n\"]\n",
    "# original_labels = [\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"B-NAME_STUDENT\",\"I-NAME_STUDENT\",\"O\"]\n",
    "max_length = 512  # Maximum token count, including special tokens like [CLS] and [SEP]\n",
    "overlap = 16  # Example overlap\n",
    "\n",
    "segments = segment_text_by_tokens(original_tokens, original_labels, max_length, overlap)\n",
    "for i, segment in enumerate(segments):\n",
    "    print(f\"Segment {i+1}:\")\n",
    "    # print(f\"Text: {segment['text']}\")\n",
    "    print(f\"Tokens: {segment['tokens']}\")\n",
    "    print(f\"Labels: {segment['labels']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'design', 'thinking', 'for', 'innovation', 'reflex', '##ion', '-', 'av', '##ril', '2021', '-', 'nat', '##hal', '##ie', 'sy', '##lla', 'challenge', '&', 'selection', 'the', 'tool', 'i', 'use', 'to', 'help', 'all', 'stakeholders', 'finding', 'their', 'way', 'through', 'the', 'complexity', 'of', 'a', 'project', 'is', 'the', 'mind', 'map', '.', 'what', 'exactly', 'is', 'a', 'mind', 'map', '?', 'according', 'to', 'the', 'definition', 'of', 'bu', '##zan', 't', '.', 'and', 'bu', '##zan', 'b', '.', '(', '1999', ',', 'des', '##sin', '##e', '-', 'moi', 'l', \"'\", 'intelligence', '.', 'paris', ':', 'les', 'editions', 'd', \"'\", 'organisation', '.', ')', ',', 'the', 'mind', 'map', '(', 'or', 'he', '##uri', '##stic', 'diagram', ')', 'is', 'a', 'graphic', 'representation', 'technique', 'that', 'follows', 'the', 'natural', 'functioning', 'of', 'the', 'mind', 'and', 'allows', 'the', 'brain', \"'\", 's', 'potential', 'to', 'be', 'released', '.', 'cf', 'annex', '##1', 'this', 'tool', 'has', 'many', 'advantages', ':', '•', 'it', 'is', 'accessible', 'to', 'all', 'and', 'does', 'not', 'require', 'significant', 'material', 'investment', 'and', 'can', 'be', 'done', 'quickly', '•', 'it', 'is', 'scala', '##ble', '•', 'it', 'allows', 'cat', '##ego', '##rization', 'and', 'linking', 'of', 'information', '•', 'it', 'can', 'be', 'applied', 'to', 'any', 'type', 'of', 'situation', ':', 'note', '##taking', ',', 'problem', 'solving', ',', 'analysis', ',', 'creation', 'of', 'new', 'ideas', '•', 'it', 'is', 'suitable', 'for', 'all', 'people', 'and', 'is', 'easy', 'to', 'learn', '•', 'it', 'is', 'fun', 'and', 'encourages', 'exchanges', '•', 'it', 'makes', 'visible', 'the', 'dimension', 'of', 'projects', ',', 'opportunities', ',', 'inter', '##con', '##ne', '##ctions', '•', 'it', 'synth', '##es', '##izes', '•', 'it', 'makes', 'the', 'project', 'understand', '##able', '•', 'it', 'allows', 'you', 'to', 'explore', 'ideas', 'the', 'creation', 'of', 'a', 'mind', 'map', 'starts', 'with', 'an', 'idea', '/', 'problem', 'located', 'at', 'its', 'center', '.', 'this', 'starting', 'point', 'generates', 'ideas', '/', 'work', 'areas', ',', 'inc', '##rem', '##ented', 'around', 'this', 'center', 'in', 'a', 'radial', 'structure', ',', 'which', 'in', 'turn', 'is', 'completed', 'with', 'as', 'many', 'branches', 'as', 'new', 'ideas', '.', 'this', 'tool', 'enables', 'creativity', 'and', 'logic', 'to', 'be', 'mobilized', ',', 'it', 'is', 'a', 'map', 'of', 'the', 'thoughts', '.', 'creativity', 'is', 'enhanced', 'because', 'participants', 'feel', 'comfortable', 'with', 'the', 'method', '.', 'application', '&', 'insight', 'i', 'start', 'the', 'process', 'of', 'the', 'mind', 'map', 'creation', 'with', 'the', 'stakeholders', 'standing', 'around', 'a', 'large', 'board', '(', 'white', 'or', 'paper', 'board', ')', '.', 'in', 'the', 'center', 'of', 'the', 'board', ',', 'i', 'write', 'and', 'highlight', 'the', 'topic', 'to', 'design', '.', 'through', 'a', 'series', 'of', 'questions', ',', 'i', 'guide', 'the', 'stakeholders', 'in', 'modelling', 'the', 'mind', 'map', '.', 'i', 'adapt', 'the', 'series', 'of', 'questions', 'according', 'to', 'the', 'topic', 'to', 'be', 'addressed', '.', 'in', 'the', 'type', 'of', 'questions', ',', 'we', 'can', 'use', ':', 'who', ',', 'what', ',', 'when', ',', 'where', ',', 'why', ',', 'how', ',', 'how', 'much', '.', 'the', 'use', 'of', 'the', '“', 'why', '”', 'is', 'very', 'interesting', 'to', 'understand', 'the', 'origin', '.', 'by', 'this', 'way', ',', 'the', 'interviewed', 'person', 'free', '##s', 'itself', 'from', 'paradigm', '##s', 'and', 'thus', 'dare', '##s', 'to', 'propose', 'new', 'ideas', '/', 'ways', 'of', 'functioning', '.', 'i', 'plan', 'two', 'hours', 'for', 'a', 'workshop', '.', 'design', 'thinking', 'for', 'innovation', 'reflex', '##ion', '-', 'av', '##ril', '2021', '-', 'nat', '##hal', '##ie', 'sy', '##lla', 'after', 'modelling', 'the', 'mind', 'map', 'on', 'paper', ',', 'i', 'propose', 'to', 'the', 'participants', 'a', 'digital', 'visual', '##ization', 'of', 'their', 'work', 'with', 'the', 'addition', 'of', 'color', 'codes', ',', 'images', 'and', 'inter', '##con', '##ne', '[SEP]']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'I-NAME_STUDENT', 'I-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'I-NAME_STUDENT', 'I-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Generate Original Token Offset Mapping\n",
    "original_token_offsets = []\n",
    "cursor = 0\n",
    "for token in tokens:\n",
    "    start = full_text.find(token, cursor)\n",
    "    end = start + len(token)\n",
    "    cursor = end\n",
    "    original_token_offsets.append((start, end))\n",
    "\n",
    "# Step 2: Tokenize and Align Labels\n",
    "encoded = tokenizer(full_text, return_offsets_mapping=True, truncation=True, padding=True)\n",
    "new_tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'])\n",
    "new_labels = ['O'] * len(new_tokens)  # Initialize all labels as 'O'\n",
    "\n",
    "for i, offset in enumerate(encoded['offset_mapping']):\n",
    "    if offset == (0, 0):  # Skip special tokens\n",
    "        continue\n",
    "    # Find the original token that encompasses the new token\n",
    "    for j, (orig_start, orig_end) in enumerate(original_token_offsets):\n",
    "        # Check if the new token is within the span of the original token\n",
    "        if orig_start <= offset[0] and offset[1] <= orig_end:\n",
    "            # Assign the original token's label to the new token\n",
    "            if orig_start == offset[0]:\n",
    "                new_labels[i] = labels[j]\n",
    "            else:\n",
    "                new_labels[i] = labels[j].replace('B-', 'I-')\n",
    "            break\n",
    "\n",
    "# Handling subwords and continuation labels: Adjust the new_labels to use continuation labels ('I-') where necessary\n",
    "for i in range(1, len(new_labels)):\n",
    "    if new_labels[i].startswith('B-') and new_labels[i-1] == new_labels[i].replace('B-', 'I-'):\n",
    "        new_labels[i] = new_labels[i].replace('B-', 'I-')\n",
    "\n",
    "# Step 3: Review the results\n",
    "print(new_tokens)\n",
    "print(new_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pii_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
